{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0J138mj7p1s"
   },
   "source": [
    "## This notebook covers how you can leverage Form Parser to generate annotations for CDW.\n",
    "Code used Form Parser form_fields and convert them to entities to gennerate annotations for CDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_v0XtSwn7fmN",
    "outputId": "373b9379-f6ac-451a-e428-ad9219fb31d0"
   },
   "outputs": [],
   "source": [
    "# Install necessary Python libraries and restart your kernel after.\n",
    "# !python -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Y8eO6Kcp7v2x"
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "# import Levenshtein\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your processor variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "k3c1mTa6IOk3"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"610517933627\"\n",
    "LOCATION = \"us\"  # Format is 'us' or 'eu'\n",
    "\n",
    "PROCESSOR_ID = \"51d19a915ff7b336\"  # Create processor in Cloud Console\n",
    "GCS_INPUT_BUCKET = 'hsbedi-docai-bucket'\n",
    "GCS_INPUT_PREFIX = 'w2/input'\n",
    "GCS_OUTPUT_URI = 'gs://hsbedi-docai-bucket'\n",
    "GCS_OUTPUT_URI_PREFIX = 'w2/output'\n",
    "GCS_OUTPUT_ANNOTATION_BUCKET = 'hsbedi-docai-bucket'\n",
    "GCS_OUTPUT_ANNOTATION_URI_PREFIX = 'w2/annotated_samples/'\n",
    "\n",
    "TIMEOUT = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code calls the batch API and stores response in output GCS location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_from_input_file():\n",
    "    \n",
    "    destination_uri = f\"{GCS_OUTPUT_URI}/{GCS_OUTPUT_URI_PREFIX}/\"\n",
    "    \n",
    "    name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/processors/{PROCESSOR_ID}\"\n",
    "    # Instantiates a client\n",
    "    client_options = {\"api_endpoint\": \"{}-documentai.googleapis.com\".format(LOCATION)}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(GCS_INPUT_BUCKET)\n",
    "    input_configs = []\n",
    "    print(\"Input Files:\")\n",
    "    counter = 0\n",
    "    api_counter = 0\n",
    "    documents = []\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=GCS_INPUT_PREFIX)\n",
    "    \n",
    "    for blob in blobs:\n",
    "        \n",
    "        counter = counter+1\n",
    "        source = \"gs://{bucket}/{name}\".format(bucket = GCS_INPUT_BUCKET, name = blob.name)\n",
    "        # print(source)\n",
    "        \n",
    "        if \".PDF\" in source.upper():\n",
    "            print(source)\n",
    "            \n",
    "            document = {\"gcs_uri\":source , \"mime_type\": \"application/pdf\"}\n",
    "            documents.append(document)\n",
    "            \n",
    "            gcs_documents = documentai.GcsDocuments(documents=documents)\n",
    "            \n",
    "            input_config = documentai.BatchDocumentsInputConfig(gcs_documents=gcs_documents)\n",
    "            input_configs.append(input_config)\n",
    "            if counter % 50 == 0:\n",
    "                \n",
    "                output_config = documentai.DocumentOutputConfig(\n",
    "                    gcs_output_config={\"gcs_uri\": destination_uri}\n",
    "                )\n",
    "                  \n",
    "                if api_counter >= 4:\n",
    "                    api_counter=0\n",
    "                    time.sleep(360)\n",
    "                    \n",
    "                request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "                name=name,\n",
    "                input_documents=input_config,\n",
    "                document_output_config=output_config,)    \n",
    "                operation = client.batch_process_documents(request)\n",
    "                print(\"process called\")\n",
    "                api_counter = api_counter + 1 \n",
    "                \n",
    "                print(input_config)\n",
    "                # print(output_config)\n",
    "\n",
    "                # Wait for the operation to finish\n",
    "#                 operation.result(timeout=TIMEOUT)\n",
    "                \n",
    "                input_configs = []\n",
    "                documents = []\n",
    "                print(counter)\n",
    "        \n",
    "    if input_configs:\n",
    "        output_config = documentai.DocumentOutputConfig(\n",
    "                    gcs_output_config={\"gcs_uri\": destination_uri}\n",
    "                )\n",
    "        request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "                name=name,\n",
    "                input_documents=input_config,\n",
    "                document_output_config=output_config,)    \n",
    "        print(input_config)\n",
    "        operation = client.batch_process_documents(request)\n",
    "        \n",
    "        print(\"process called out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Call Form Parser to batch process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document_from_input_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_field_name(name,demiliter='_'):\n",
    "    CDE_field_dict = {'A_EMPLOYEES_SOCIAL_SECURITY_NUMBER': 'EMPL_SSN', 'B_EMPLOYER_IDENTIFICATION_NUMBER': 'EMPLR_ID_NUMBER', 'C_EMPLOYERS_NAME_ADDRESS_AND_ZIP_CODE': 'EMPLR_NAME_ADDRESS','D_CONTROL_NUMBER':'CONTROL_NUMBER',\n",
    "                        '1_WAGES_TIPS_OTHER_COMPENSATION': 'WAGES_TIPS_OTHER_COMP', '2_FEDERAL_INCOME_TAX_WITHHELD':'FEDERAL_INCOME_TAX_WH', '3_SOCIAL_SECURITY_WAGES':'SS_WAGES', '4_SOCIAL_SECURITY_TAX_WITHHELD':'SS_TAX_WH' }\n",
    "    name = name.strip()\n",
    "    name = name.replace('\\n',' ')\n",
    "    name = name.replace(',','')\n",
    "    name = name.replace(\"'\",'')\n",
    "    name = name.replace('  ',' ')\n",
    "    name = name.upper()\n",
    "    name = name.replace(' ',demiliter)\n",
    "    \n",
    "    if name in CDE_field_dict:\n",
    "        return CDE_field_dict[name]\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "def create_entity(form_field_name,form_field_value,form_textSegments,form_boundingPoly):\n",
    "    \n",
    "    entity_field_name = format_field_name(form_field_name)\n",
    "    \n",
    "    if entity_field_name:\n",
    "        entity = {}\n",
    "        entity['mentionText'] = form_field_value\n",
    "        entity['type'] = entity_field_name\n",
    "        \n",
    "        normalizedVertices = []\n",
    "        for vertex in form_boundingPoly.normalized_vertices:\n",
    "            x= vertex.x\n",
    "            y= vertex.y\n",
    "            normalizedVertices.append({\"x\":x,\"y\":y})\n",
    "        \n",
    "        pageRefs = []\n",
    "        pageRefs.append({\"boundingPoly\":{\"normalizedVertices\":normalizedVertices}})\n",
    "        entity['pageAnchor'] = {\"pageRefs\":pageRefs}\n",
    "        \n",
    "        \n",
    "        textSegments = []\n",
    "        for segment in form_textSegments:\n",
    "            textSegments.append({\"endIndex\":segment.end_index,\"startIndex\":segment.start_index})\n",
    "        entity['textAnchor'] = {\"content\":form_field_value,\"textSegments\":textSegments}\n",
    "        \n",
    "        return entity\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "   \n",
    "def entity_from_formfield(form_field):\n",
    "    \n",
    "    field_name = form_field.field_name.text_anchor.content\n",
    "    field_value = form_field.field_value.text_anchor.content\n",
    "    boundingPoly = form_field.field_value.bounding_poly\n",
    "    textSegments = form_field.field_value.text_anchor.text_segments\n",
    "    entity = create_entity(field_name,field_value,textSegments,boundingPoly)\n",
    "    return entity\n",
    "\n",
    "def generate_entities_from_form_fields(document):\n",
    "    entities = None\n",
    "    for page in document.pages:\n",
    "        for form_field in page.form_fields:\n",
    "            entity = entity_from_formfield(form_field)\n",
    "            # print(entity)\n",
    "            if entity:\n",
    "                if not entities:\n",
    "                    entities = []\n",
    "                entities.append(entity)\n",
    "    print(entities)\n",
    "    return entities\n",
    "\n",
    "def parse_sample_files_in_gcsbucket_mod():\n",
    "  \n",
    "    destination_uri = f\"{GCS_OUTPUT_URI}/{GCS_OUTPUT_URI_PREFIX}/\"\n",
    "    # Results are written to GCS. Use a regex to find\n",
    "    # output files\n",
    "    match = re.match(r\"gs://([^/]+)/(.+)\", destination_uri)\n",
    "    output_bucket = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(output_bucket)\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "\n",
    "    for i, blob in enumerate(blob_list):\n",
    "        # If JSON file, download the contents of this blob as a bytes object.\n",
    "        lineindex=-1\n",
    "        if \".json\" in blob.name:\n",
    "            match = re.match(r\"(.+)-(\\d).json\", blob.name.split(\"/\")[-1])\n",
    "            output_file_name = match.group(1)\n",
    "            print(output_file_name)\n",
    "            blob_as_bytes = blob.download_as_string()\n",
    "            print(\"downloaded\")\n",
    "\n",
    "            document = documentai.types.Document.from_json(blob_as_bytes)\n",
    "            document_json = json.loads(blob_as_bytes)\n",
    "            print(f\"Fetched file {i + 1}\")\n",
    "            entities = generate_entities_from_form_fields(document)\n",
    "            document_json[\"entities\"] = entities\n",
    "            \n",
    "            create_json(document_json,output_file_name)\n",
    "            \n",
    "\n",
    "def create_json(json_object, filename):\n",
    "    '''\n",
    "    this function will create json object in\n",
    "    google cloud storage\n",
    "    '''\n",
    "    # create a blob\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(GCS_OUTPUT_ANNOTATION_BUCKET)\n",
    "    blob = bucket.blob\n",
    "    blob = bucket.blob(GCS_OUTPUT_ANNOTATION_URI_PREFIX+filename+'.json')\n",
    "    # upload the blob \n",
    "    blob.upload_from_string(\n",
    "        data=json.dumps(json_object),\n",
    "        content_type='application/json'\n",
    "        )\n",
    "    result = filename + ' upload complete'\n",
    "    return {'response' : result}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Read output json from Form parser to generate doc proto for CDW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sample_files_in_gcsbucket_mod()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LendingAI Bouding Boxes v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
